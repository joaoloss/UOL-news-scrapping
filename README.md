# UOL-news-scrapping

## üìù About

This program was developed as part of an extension program at the [Neologism's laboratory](https://www.neoscopio.com.br/) at [UFES](https://www.ufes.br/), with non-commercial goals. Its main objective is to create a large corpus of text from UOL news for identifying [neologisms](https://en.wikipedia.org/wiki/Neologism) in the Brazilian Portuguese language. The whole process is based on [web scraping](https://pt.wikipedia.org/wiki/Web_scraping) techniques.

Originally it was developed to extract news from the beginning of 2010 up to the end of 2019 (a decade of news).

## üêç Core Python libs

- [logging](https://docs.python.org/3/library/logging.html): for efficient and structured logs.
- [requests](https://requests.readthedocs.io/en/latest/): for HTTP requests.
- [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html): for multithreading processing.
- [selenium](https://selenium-python.readthedocs.io/): for interaction with dynamic page where the browser is necessary (used just in `archive_links_extraction.py`).
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): for interaction with static page, where the HTTP protocol is sufficient intermediate - no need of browsers.

## ‚öôÔ∏è Workflow description

The entire extraction process can be divided into 3 major steps.

### 1. Archive links extraction

In this stage, the goal is to collect all available UOL news homepage links from the [Wayback Machine archive](https://help.archive.org/help/using-the-wayback-machine/) for the specified date range.

This process is done by `archive_links_extraction.py` script.

### 2. UOL news link extraction

Once the news home pages are collected, we can scrape each of these pages in order to get the actual news links, references by [HTML anchor elements](https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/a) in the home pages.

This process is done by `uol_links_extraction.py` script.

### 3. UOL news text extraction

Finally, once we have the news links from the previous step, we scrape and clean the news text from all collected links.

This process is done by `uol_news_extraction.py` script.

### Note

There is a small description of the script workflow at the top of each script file.

## üíª Usage

### 1. Cloning

```bash
git clone https://github.com/joaoloss/UOL-news-scrapping.git
cd UOL-news-scrapping
```

### 2. Virtual enviroment configuration

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### 3. Running archive links extraction script

```bash
python3 archive_links_extraction.py [-h] [--headless] --start-date START_DATE --end-date END_DATE [--quiet]
```

- `--headless`: If enabled, the program will run without a graphical user interface (headless mode).
- `--start-date`: Specify the start date to collect links in `mm/yyyy` format. Example: `--start-date 01/2010`.
- `--end-date`: Specify the end date to collect links in `mm/yyyy` format. Example: `--end-date 12/2019`.
- `--quiet`: If enabled, it will suppress the logs will not be displayed in the terminal.

### 4. Running uol links extraction script

```bash
python3 uol_links_extraction.py [-h] [--quiet]
```

- `--quiet`: If enabled, it will suppress the logs will not be displayed in the terminal.

### 5. Running uol news extraction script

```bash
python3 uol_news_extraction.py [-h] [--quiet] --year-folder YEAR_FOLDER
```

- `--quiet`: If enabled, it will suppress the logs will not be displayed in the terminal.
- `--year-folder`: Year from which news will be scraped. It should be the name of the corresponding folder in out/uol_links generated by the previous script. Example: `--year-folder 2019`.